# Experimentation 2 : üîÑ Drainage d'un noeud
> üö® Avant tout chose v√©rifiez si vous √™tes bien dans le bon contexte, √† savoir celui de **minikube** toujours √† l'aide de la commande :
````
kubectl config get-contexts
````

Supprimez le namespace de la premi√®re experimentation
````
kubectl delete namespace experimentation1
````

## Contexte

Pour cette seconde experimentation voici le contexte dans lequel vous allez interagir.

![contexte](docs/contexte.png)


L'√©quipe m√©tier celle qui g√®re l'application souhaite que son application soit disponible **√† tout prix** !
Aucune indisponiblit√© ne doit survenir, en effet aujourd'hui c'est le lancement de son nouveau produit !

Vous faites partie de l'√©quipe qui g√®re le cluster kubernetes c'est donc de votre ressort de vous assurer qu'aucune indisponibilit√© ne survienne.


### Instructions

Cr√©er un namespace appel√© *experimentation2* :

````
kubectl create namespace experimentation2
````

Voici la sortie de la console attendue :

````
namespace/experimentation2 created
````

Placez vous dans le dossier *experimentation2*  :
````
cd experimentation2
````

Placez vous dans ce namespace *experimentation2* :
````
kubectl config set-context --current --namespace=experimentation2
````

Vous pouvez appliquer les fichiers de ressources mis √† votre disposition il y'a un deployment et un service, √† l'aide de la commande :
`````
kubectl apply -f deployment.yaml && kubectl apply -f service.yaml
`````

V√©rifier que les 6 pods sont √† l'√©tat running soit au travers de la cli soit au travers du dashboard (que vous avez ouvert pr√©c√©dement) :

`````
kubectl get pods -owide
`````

![Step 1](docs/experimentation2_step_1.png)


R√©cup√©rer l'url via cette commande, garder l'url car il faudra la renseigner dans le fichier de l'experimentation 2 √† la ligne 30 [experiment_2_draining_nodes](./experiment_2_draining_nodes.yaml)  :

````
minikube service list -n experimentation2
````

|    NAMESPACE     |    NAME    | TARGET PORT |            URL             |
|------------------|------------|-------------|----------------------------|
| experimentation2 | my-service |        8080 | url √† r√©cup√©rer et √† coller √† la ligne 30 du fichier [experiment_2_draining_nodes](./experiment_2_draining_nodes.yaml) |


Vous devez arriver sur une page similaire √† celle-ci :
![pod](docs/app2.png)

Regarder commment est constitu√© le fichier [experiment_2_draining_nodes](./experiment_2_draining_nodes.yaml).
N'oubliez pas de remplacer l'url √† la ligne 30 par la votre !

Vous l'aurez remarqu√© nous avons 1 action de drainage de noeud qui va √™tre jou√©e !

L'objectif c'est d'observer lorsque cette action de drainage surviendra comment cela impactera notre l'application.


Vous allez pouvoir en premier lieu valider la syntaxe de la seconde exp√©rimentation √† l'aide de la commande :
````
chaos validate experiment_2_draining_nodes.yaml
````

Puis vous allez pouvoir lancer l'exp√©rimentation, sans activer la proc√©dure de rollback volontairement pour observer ce qu'il se passe :
````
chaos run experiment_2_draining_nodes.yaml
````

Regardez ce qu'il se passe c√¥t√© dashboard, il semble que notre deploiement a √©t√© fortement impact√© :
![deployment_down](docs/down.png)


Si vous observez l'√©tat de vos noeuds √† l'aide de la commande :
````
kubectl get nodes  -l=role=worker
````

Vous constaterez qu'ils sont √† l'√©tat *Ready* mais avec le status suppl√©mentaire *SchedulingDisabled*, c'est du au fait de l'action *drain_nodes* :
````
NAME           STATUS                     ROLES    AGE   VERSION
minikube-m02   Ready,SchedulingDisabled   <none>   30m   v1.23.0
minikube-m03   Ready,SchedulingDisabled   <none>   29m   v1.23.0
````

Du c√¥t√© de la sortie de la console vous aurez une sortie similaire :
````
[INFO] Validating the experiment's syntax
[INFO] Experiment looks valid
[INFO] Running experiment: Mon application est r√©siliente au drainage de noeuds
[INFO] Steady-state strategy: default
[INFO] Rollbacks strategy: default
[INFO] Steady state hypothesis: Les services sont tous disponible et en bonne sant√©
[INFO] Probe: pods_in_phase
[INFO] Probe: ma-super-app-repond-normalement
[INFO] Steady state hypothesis is met!
[INFO] Playing your experiment's method now...
[INFO] Action: drain_nodes
[INFO] Steady state hypothesis: Les services sont tous disponible et en bonne sant√©
[INFO] Probe: pods_in_phase
[ERROR]   => failed: chaoslib.exceptions.ActivityFailed: pod 'biz-app-id=retail' is in phase 'Pending' but should be 'Running'
[WARNING] Probe terminated unexpectedly, so its tolerance could not be validated
[CRITICAL] Steady state probe 'pods_in_phase' is not in the given tolerance so failing this experiment
[INFO] Experiment ended with status: deviated
[INFO] The steady-state has deviated, a weakness may have been discovered
````

Proc√©dez √† l'op√©ration de rollback manuellement :
````
kubectl uncordon -l=role=worker
````

Afin de relancer l'exp√©remimentation dans sa totalit√© en incluant l'op√©ration de rollback, lancez la commande suivante :
````
chaos run experiment_2_draining_nodes.yaml --rollback-strategy=always
````

Que s'est t'il pass√© durant cette experimentation ?
>

# üéâ F√©licitations vous avez observ√© une faiblesse ! Il y'a bien un probl√®me dans la configuration actuelle !

Que pourrions nous faire *tr√®s simplement* pour √©viter ce genre de situation ?
> La r√©ponse est de mettre en place la ressource kubernetes nomm√©e P _ D - D _ _ _ _ _ _ _ _ _ _ N - B _ _ _ _ T

La solution se trouve ici [solution d√©taill√©e](solution/README.md)
